{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afcc1ad6",
   "metadata": {},
   "source": [
    "1. Write the Python code to implement a single neuron.\n",
    "\n",
    "Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c3e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, input_size):\n",
    "        self.weights = np.random.rand(input_size)\n",
    "        self.bias = np.random.rand()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        z = np.dot(inputs, self.weights) + self.bias\n",
    "        a = self.activation_function(z)\n",
    "        return a\n",
    "    \n",
    "    def activation_function(self, x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14a4c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a neuron with 2 inputs\n",
    "neuron = Neuron(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e3552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input values from the user\n",
    "input1 = float(input(\"Enter input 1: \"))\n",
    "input2 = float(input(\"Enter input 2: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1943cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction based on the inputs\n",
    "inputs = np.array([input1, input2])\n",
    "output = neuron.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640038d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The predicted output is:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d608a",
   "metadata": {},
   "source": [
    "2. Write the Python code to implement ReLU.\n",
    "\n",
    "Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91b9504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25241ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply ReLU to the input value\n",
    "output = relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d41c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The output of ReLU applied to\", x, \"is:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d344b5",
   "metadata": {},
   "source": [
    "3. Write the Python code for a dense layer in terms of matrix multiplication.\n",
    "\n",
    "Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38150648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ba310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size)\n",
    "        self.bias = np.random.rand(output_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        z = np.dot(inputs, self.weights) + self.bias\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e8a5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dense layer with 2 inputs and 3 outputs\n",
    "layer = DenseLayer(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f50cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input values from the user\n",
    "input1 = float(input(\"Enter input 1: \"))\n",
    "input2 = float(input(\"Enter input 2: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75c4698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction based on the inputs\n",
    "inputs = np.array([input1, input2])\n",
    "output = layer.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2f013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The predicted output is:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45988935",
   "metadata": {},
   "source": [
    "4. Write the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python).\n",
    "\n",
    "Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d26e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df52686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = [[random.random() for _ in range(output_size)] for _ in range(input_size)]\n",
    "        self.bias = [random.random() for _ in range(output_size)]\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        z = [sum([inputs[i] * self.weights[i][j] for i in range(len(inputs))]) + self.bias[j] for j in range(len(self.bias))]\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3918a08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dense layer with 2 inputs and 3 outputs\n",
    "layer = DenseLayer(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675bd391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input values from the user\n",
    "input1 = float(input(\"Enter input 1: \"))\n",
    "input2 = float(input(\"Enter input 2: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f457f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction based on the inputs\n",
    "inputs = [input1, input2]\n",
    "output = layer.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07acc0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The predicted output is:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0637a933",
   "metadata": {},
   "source": [
    "5. What is the “hidden size” of a layer?\n",
    "\n",
    "Ans: The \"hidden size\" of a layer refers to the number of neurons in that layer. In other words, it is the number of nodes in the layer that are not directly connected to the input or output layers.\n",
    "\n",
    "For example, in a neural network with an input layer, an output layer, and a single hidden layer, the hidden size of that layer would refer to the number of neurons in the hidden layer. The input and output layers have a fixed size determined by the input and output of the task being performed, but the number of neurons in the hidden layer can vary and is typically chosen by the user based on the complexity of the task.\n",
    "\n",
    "The hidden size of a layer is an important hyperparameter that can affect the performance of a neural network. A larger hidden size may allow the network to learn more complex features, but may also increase the risk of overfitting. On the other hand, a smaller hidden size may simplify the network and reduce the risk of overfitting, but may not be able to learn as complex of features.\n",
    "\n",
    "6. What does the t method do in PyTorch?\n",
    "\n",
    "Ans:\n",
    "\n",
    "In PyTorch, the t() method is used to transpose a tensor.\n",
    "\n",
    "A tensor in PyTorch is a multi-dimensional array, similar to a NumPy array. Transposing a tensor means to swap its rows and columns, effectively rotating the tensor. For example, if we have a 2-dimensional tensor t with shape (3, 4), transposing it would result in a tensor with shape (4, 3).\n",
    "\n",
    "Here's an example of using the t() method in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a0ca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eaccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 2-dimensional tensor\n",
    "t = torch.tensor([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabb211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the original tensor\n",
    "print(\"Original tensor:\")\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a3c143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose the tensor\n",
    "t_transposed = t.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2115ce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the transposed tensor\n",
    "print(\"Transposed tensor:\")\n",
    "print(t_transposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2203d81e",
   "metadata": {},
   "source": [
    "7. Why is matrix multiplication written in plain Python very slow?\n",
    "\n",
    "Ans: Matrix multiplication involves a large number of arithmetic operations, and performing those operations using the basic Python operations can be slow because Python is an interpreted language, which means that it has to interpret and execute each line of code at runtime. This can lead to a significant overhead when working with large matrices.\n",
    "\n",
    "Moreover, the built-in data structures in Python, such as lists or tuples, are not optimized for numerical operations and lack the efficient memory access patterns that are required for efficient matrix multiplication.\n",
    "\n",
    "In contrast, specialized numerical libraries such as NumPy or TensorFlow are written in optimized C or Fortran code, which allows for much faster execution of numerical operations. They also offer various optimizations such as parallelism, vectorization, and caching, which further improve the performance of matrix multiplication.\n",
    "\n",
    "Therefore, if you need to perform matrix multiplication, it's recommended to use a specialized library such as NumPy, which provides optimized functions for matrix operations and can execute them much faster than plain Python.\n",
    "\n",
    "8. In matmul, why is ac==br?\n",
    "\n",
    "Ans: In matrix multiplication, two matrices A and B can be multiplied only if the number of columns of A (denoted by 'c') is equal to the number of rows of B (denoted by 'r').\n",
    "\n",
    "When we write matmul(A,B), A is assumed to be a matrix of shape (a, c) and B is assumed to be a matrix of shape (c, b). The resulting matrix C from the matrix multiplication of A and B would have the shape (a, b).\n",
    "\n",
    "The element at row 'i' and column 'j' in C can be calculated by taking the dot product of the i-th row of A and the j-th column of B. This requires that the length of the row in A (i.e., 'c') must be equal to the length of the column in B (also 'c'). Therefore, we have the requirement that ac == br in order to perform matrix multiplication using matmul.\n",
    "\n",
    "In summary, the requirement ac == br ensures that the matrices can be multiplied according to the rules of matrix multiplication, and that the resulting matrix has the expected shape.\n",
    "\n",
    "9. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
    "\n",
    "Ans: To measure the time taken for a single cell to execute in a Jupyter Notebook, you can use the %%timeit magic command. This command runs the cell multiple times and provides the average time taken for the cell to execute.\n",
    "\n",
    "To use the %%timeit command, you simply need to add it at the beginning of the cell you want to time. For example:\n",
    "\n",
    "%%timeit\n",
    "\n",
    "#Your code here\n",
    "\n",
    "Once you run the cell with the %%timeit command, Jupyter Notebook will execute the cell multiple times with different inputs and record the time taken for each execution. It will then calculate the average time taken and display it along with other statistics such as standard deviation and confidence intervals.\n",
    "\n",
    "Alternatively, you can also use the %time command to measure the time taken for a single execution of a cell. This command provides more detailed information than %%timeit, but it only runs the cell once. To use %time, simply add it at the beginning of the cell you want to time, like this:\n",
    "\n",
    "%time\n",
    "\n",
    "#Your code here\n",
    "\n",
    "After running the cell with %time, Jupyter Notebook will display the time taken for the cell to execute, along with other detailed information about the execution, such as CPU time and memory usage.\n",
    "\n",
    "10. What is elementwise arithmetic?\n",
    "\n",
    "Ans: Elementwise arithmetic refers to performing arithmetic operations on the corresponding elements of two or more arrays or matrices. In elementwise arithmetic, each element of one array is paired with the corresponding element of another array, and the arithmetic operation is performed on those pairs of elements.\n",
    "For example, given two arrays A and B, elementwise addition would involve adding each element of A to the corresponding element of B, resulting in a new array C where each element is the sum of the corresponding elements in A and B.\n",
    "\n",
    "Elementwise arithmetic is commonly used in numerical computations, particularly in linear algebra and machine learning. In Python, libraries such as NumPy provide efficient support for elementwise arithmetic operations on arrays and matrices.\n",
    "\n",
    "Elementwise arithmetic is different from matrix multiplication, where each element in the resulting matrix is the sum of the products of the corresponding elements of two input matrices. In matrix multiplication, the input matrices must have compatible shapes, whereas in elementwise arithmetic, the input arrays must have the same shape.\n",
    "\n",
    "11. Write the PyTorch code to test whether every element of a is greater than the corresponding element of b.\n",
    "\n",
    "Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded575f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a69ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([0, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cbe088",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.gt(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf07c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.all(result):\n",
    "    print(\"All elements of a are greater than b\")\n",
    "else:\n",
    "    print(\"Not all elements of a are greater than b\")\n",
    "    print(\"Indices where the condition fails: \", torch.where(result == False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d81fa7",
   "metadata": {},
   "source": [
    "12. What is a rank-0 tensor? How do you convert it to a plain Python data type?\n",
    "\n",
    "Ans: A rank-0 tensor is a tensor with zero dimensions, also known as a scalar or a 0-d tensor. It represents a single numerical value, such as an integer or a floating-point number.\n",
    "\n",
    "In PyTorch, a rank-0 tensor is created using the torch.tensor() function with a single argument, which is the scalar value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728c5097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(42)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7778aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935fc3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(42)\n",
    "y = x.item()\n",
    "print(type(y))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb05c78",
   "metadata": {},
   "source": [
    "13. How does elementwise arithmetic help us speed up matmul?\n",
    "\n",
    "Ans: Element-wise arithmetic can help speed up matrix multiplication by reducing the number of operations required to compute the result. Specifically, element-wise arithmetic can be used to implement a faster version of matrix multiplication known as “broadcasting matrix multiplication.”\n",
    "\n",
    "In broadcasting matrix multiplication, the two matrices being multiplied are broadcasted to have the same shape. This allows us to perform element-wise multiplication between the two matrices, followed by a sum along one of the axes to compute the final result.\n",
    "\n",
    "This approach can be much faster than traditional matrix multiplication because it takes advantage of the parallel processing capabilities of modern hardware. By performing many element-wise operations in parallel, we can significantly reduce the time it takes to compute the result of a matrix multiplication.\n",
    "\n",
    "Here’s an example that shows how element-wise arithmetic can be used to implement a faster version of matrix multiplication:\n",
    "\n",
    "14. What are the broadcasting rules?\n",
    "\n",
    "Ans: Broadcasting is a powerful mechanism in NumPy and PyTorch that allows you to perform operations between tensors of different shapes. The broadcasting rules determine how tensors with different shapes are treated during arithmetic operations.\n",
    "\n",
    "Here are the broadcasting rules:\n",
    "\n",
    "If the two tensors have different numbers of dimensions, the shape of the tensor with fewer dimensions is padded with ones on its leading (left) side.\n",
    "\n",
    "If the shape of the two tensors does not match in any dimension, the tensor with shape equal to 1 in that dimension is stretched to match the other shape.\n",
    "\n",
    "If in any dimension the sizes disagree and neither is equal to 1, an error is raised.\n",
    "\n",
    "These rules allow you to perform element-wise operations between tensors of different shapes in a flexible and intuitive manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95acb21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create two matrices\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "# Compute the result of matrix multiplication using broadcasting and element-wise arithmetic\n",
    "result = (a.unsqueeze(-1) * b.unsqueeze(0)).sum(dim=1)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b780f0",
   "metadata": {},
   "source": [
    "In this example, we use unsqueeze to add an extra dimension to a and b, allowing us to broadcast them to have the same shape. We then perform element-wise multiplication between a and b, followed by a sum along dimension 1 to compute the final result. This approach is much faster than traditional matrix multiplication for large matrices.\n",
    "\n",
    "15. What is expand_as? Show an example of how it can be used to match the results of broadcasting.\n",
    "\n",
    "Ans: expand_as is a PyTorch tensor method that allows you to expand a tensor to the size of another tensor. This can be useful when you want to perform operations between tensors of different sizes that are not broadcastable.\n",
    "\n",
    "Here’s an example that shows how expand_as can be used to match the results of broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233fefa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create two tensors of different sizes\n",
    "x = torch.tensor([[1], [2], [3]])\n",
    "y = torch.tensor([4, 5, 6])\n",
    "\n",
    "# Use expand_as to expand x to the size of the result of broadcasting x and y\n",
    "x_expanded = x.expand_as(x + y)\n",
    "\n",
    "# Perform element-wise addition between x_expanded and y\n",
    "result = x_expanded + y\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3513d44e",
   "metadata": {},
   "source": [
    "In this example, x is a 3x1 tensor and y is a 1D tensor of size 3. The expand_as method is used to expand x to the size of the result of broadcasting x and y. This allows us to perform element-wise addition between x_expanded and y, producing the same result as if we had broadcasted x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8523f5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
